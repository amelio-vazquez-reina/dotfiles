#!zsh
## Apply the command recursively on an ABSOLUTE PATH
# pathargs PATH COMMAND
# e.g.: pathargs `pwd` chmod o+rx

# aws ec2 describe-instances --instance-id $1 | jq '.Reservations[].Instances[].PrivateIpAddress' --raw-output

function load-plugin() {
  source $ZSH/plugins/"$1"/"$1".plugin.zsh
}

function print_path()
{
    tr ':' '\n' <<< "$PATH"
}
function test_if_ends_in_newline()
{
    test `tail -c 1 $1` && echo "no newline at eof: $1"
}

function qsubtest ()
{


}

function qhivesub ()
{
    query_parent_folder=${1:h}
    query_file_name=${1:t:r}

    p_query=$PATH_TO_OPTIMIZATION_SANDBOX_REPOSITORY/utils/queries/qubole/$1
    p_out=$PATH_TO_DATA/qubole/$query_parent_folder
    mkdir -p $p_out
    gopy2
    echo $query_file_name
    qds.py --token=$QUBOLE_KEY_AVAZQUEZ hivecmd run --script_location $p_query --tags "Team=opt-team" --cluster-label "default" > $p_out/$query_file_name"_"$2.csv
}

function qbr ()
{
    qds.py --token=$QUBOLE_KEY_AVAZQUEZ hivecmd getresult "$*"
}

function concat_csvs()
{
awk '
    NR == 1 {print}
    FNR == 1 {next}
    {print}
' $1/*.csv > $2
}

function sort_all_files_by_size()
{
    find . -type f  -exec du -h {} + | sort -r -h
}

function sort_du_by_size()
{
    du -sk * | sort -g | awk '{ numBytes = $1 * 1024; numUnits = split("B K M G T P", unit); num = numBytes; iUnit = 0; while(num >= 1024 && iUnit + 1 < numUnits) { num = num / 1024; iUnit++; } $1 = sprintf( ((num == 0) ? "%6d%s " : "%6.1f%s "), num, unit[iUnit + 1]); print $0; }'
}

function sort_du_by_size_slower()
{
    du | sort -nr | cut -f2- | xargs du -hs
}

################################################################
function backup_init_files()
{
    current_dir=$(pwd)
    cd /Users/amelio/code/cnc && ga -A; gc -am "Backup" && gp
    cd $current_dir
}

function code_backup()
{
    current_dir=$(pwd)
    cd /Users/amelio/code/cnc/my_code && ./cp_and_clean_my_code.sh
    cd $current_dir
    backup_init_files
}
################################################################
function get_file_size_by_extension()
{
    ftypes=$(find . -type f | grep -E ".*\.[a-zA-Z0-9]*$" | sed -e 's/.*\(\.[a-zA-Z0-9]*\)$/\1/' | sort | uniq)

    for ft in $ftypes
    do
        echo -n "$ft "
        gfind . -name "*${ft}" -exec ls -l {} \; | gawk '{total += $5} END {print total}'
    done
}

function update_conda_specs()
{
     conda list -e > $PATH_TO_OPTIMIZATION_SANDBOX_REPOSITORY/environments/conda_latest_spec_file.txt

}
function crpos()
{
    cdev
    echo "Commiting changes locally ..."
    ga -A; gc -am $1;
    echo "Pushing ..."
    git pull --rebase; gp
    echo "ssh and update ..."
    ssh -t dataxu@dx.an04.sldc "export LC_STARTUP='crosgitpull'; /var/www/analyticsDashboard/dataFiles/bin/install/bin/zsh;"
}

function build_and_upload_lc_report()
{
	 cd $PATH_TO_LEARNING_CONFIG_REPOSITORY
   mvn clean install -U
	 git_branch=$(git rev-parse --abbrev-ref HEAD)
   rsync -rv --progress $PATH_TO_LEARNING_CONFIG_REPOSITORY/archive/config-report-test/mcma_config_report.txt -e ssh dx.an04.sldc:/var/www/analyticsDashboard/dataFiles/learning-config_build_reports/${git_branch}.txt
	 printf "The report is available from:\n"
	 printf "http://analytics04.sldc.dataxu.net/dataFiles/learning-config_build_reports/${git_branch}.txt\n"
}

function upload_lc_report()
{
	 cd $PATH_TO_LEARNING_CONFIG_REPOSITORY
	 git_branch=$(git rev-parse --abbrev-ref HEAD)
	 # mvn install -P integration
   rsync -arv --progress $PATH_TO_LEARNING_CONFIG_REPOSITORY/archive/config-report-test/mcma_config_report.txt -e ssh dx.an04.sldc:/var/www/analyticsDashboard/dataFiles/learning-config_build_reports/${git_branch}.txt
	 printf "The report is available from:\n"
	 printf "http://analytics04.sldc.dataxu.net/dataFiles/learning-config_build_reports/${git_branch}.txt\n"
}

function remove_first_m_last_n()
{
		awk -v m=$1 -v n=$2 'NR<=m{next};NR>n+m{print line[NR%n]};{line[NR%n]=$0}'
}
function sample_from_csv_file()
{
		# E.g. Get 10K lines from one csv file:
		# sample_from_csv_file src.csv target.csv 10000
		head -n 1 $1 > $2 && cat $1 | remove_first_m_last_n 1 1 | gshuf | head -n $3 >> $2
}
function aws_IP() {
aws ec2 describe-instances --instance-id $1 --query 'Reservations[].Instances[].PrivateIpAddress' | jq '.[]' --raw-output
}

function start_and_ssh()
{
    aws ec2 start-instances --instance-ids $1 > /dev/null
    ssh ec2-user@$(aws_IP $1) -i ~/.ssh/aws/amelio.pem
}

function join_on_disk
{
		# Useful to concatenate multiple csv files
		for i in <->; do
				cat $i || break
		done > $1
}
function cat_lines
{
    tail -n +$1 $3 | head -n $(($2-$1+1))
}

function pathargs
{
    P=$1; shift;
    while [[ -n $P ]]; do echo $P; P=${P%/*}; done \
	| tac | xargs -d '\n' "$@"
}

# Show the full path to a file

function show_path
{
    P=$1;
		if [[ $(dirname $P) == '.' ]] then
			 printf "$(pwd)/$P\n"
		else
			 printf "$(pwd)/$(dirname $P)/$P\n"
		fi

}

function get_history_with_timestamp
{
		perl -lne 'm#: (\d+):\d+;(.+)# && printf "%s :: %s\n",scalar localtime $1,$2' $HISTFILE
}


# function diff {
#      colordiff -u "$@"
#  }

# Find any partial matches under the following directory:
function find1
{
    P=$1;
    find -maxdepth 1 -iname "*$1*" 2>/dev/null
}

# Find any partial or full matches (any depth)
function findn
{
    P=$1;
    find . -iname "*$1*" 2>/dev/null
}

function findnl
{
    P=$1;
    find -L . -iname "*$1*" 2>/dev/null
}


function u2w
{
    readlink -f "$1" | sed 's|^/nfs|\\|;s|/|\\|g'
}

function cdd()
{
		cd $(dirname $1)
}

function w2u
{
    # This one works in Zsh:
    print -r -- "$1" | sed 's@^\\@/nfs@;s@\\@/@g'

    # See this thread: http://unix.stackexchange.com/questions/70459/treatment-of-backslashes-across-shells

    # Other attempts:
    # echo $1 | sed 's@^\\@/nfs/@;s@\\@/@g'
    # printf '$s' $1
}

between() {
  perl -lne 'BEGIN{$b=shift;$e=shift}
             print for /\Q$b\E(.*?)\Q$e\E/g' "$@"
}

sort_git_changes_by_date () {
		for file in */**; do echo $(git log --pretty=format:%ad -n 1 --date=short -- $file) $file; done < <(git ls-tree -r --name-only HEAD) | sort -k1,1n
}

date_from_timestamp_in_argument () {
		date -d @$(echo $(( $1/1000 )))
}


date_from_ms_timestamp () {
		date -d @$(echo $(( $TIMESTAMP/1000 )))
}


get_latest_rc_timestamp_on_rts_updater_HR () {
		# TIMESTAMP in ms
		# Output should be human readable
		TIMESTAMP=$(ssh rts-updater01.sliad.dataxu.net "echo /opt/dxrts/fs-root/Updater/rate-curve/*.tar.bz2"  | awk -F'ratecurves-' '{print $2}' | sed s/.tar.bz2//g)
		date -d @$(echo $(( $TIMESTAMP/1000 ))) +%Y-%m-%d-%A-%H.%M.%S
}


get_latest_xude_timestamp_on_rts_updater_HR () {
		# TIMESTAMP in ms
		# Output should be human readable
		TIMESTAMP=$(ssh rts-updater01.sliad.dataxu.net "echo  /opt/dxrts/fs-root/Updater/fcc-evalue/*.tar.bz2"  | gsed -r 's|^.*fcc-evalue/([0-9]+).*|\1|')
		date -d @$(echo $(( $TIMESTAMP/1000 ))) +%Y-%m-%d-%A-%H.%M.%S
}


get_latest_rc_timestamp_on_analytics_HR () {
		# TIMESTAMP in seconds
		# Output should be human readable
		TIMESTAMP=$(ssh dataxu@analytics04.sldc.dataxu.net "date +%s -r /var/www/analyticsDashboard/scripts/rateCurve3Dv3/plotRateCurveOutput.csv")
		date -d @$(echo $(( $TIMESTAMP ))) +%Y-%m-%d-%A-%H.%M.%S
}

show_current_time() {
		date +%Y-%m-%d-%A-%H.%M.%S
}

show_file_commit_timestamps() {
		# Display the timestamps of the files in the folder
		git ls-tree -r --name-only HEAD | while read filename; do
				echo "$(git log -1 --format="%ai" -- $filename) $filename"
		done | sort
}

show_ip_address() {
    ifconfig | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*' | grep -Eo '([0-9]*\.){3}[0-9]*' | grep -v '127.0.0.1'
}
show_filename_count_per_commit() {
		for (( x = 0; x <= $1; x++ )); do
				print $(git show -s --format=%B HEAD~$x);
				git diff HEAD~$x HEAD~$(( $x + 1 )) --name-only | awk 'BEGIN{FS="/"}{print $NF}' | sort | uniq -c;
		done
}

pull_pacing_config() {
		(cd $PATH_TO_PACING_CONFIGURATIONS_REPOSITORY && git pull)

}

## DATAXU
retrieve_flights_with_custom_calibration() {
	  (cd $PATH_TO_LEARNING_CONFIG_REPOSITORY
    find . -name 'properties.txt' -exec grep -iF 'calibration' {} + | grep 'flightUID' | gsed -r 's/^.*flightUID=(0F.{0,8}).*/\1/' | sort -u)

}
################################################################
mount_data_cache() {

		# Mounting our network drive:
		# echo "Mounting avazquez @ dx-bos-file01 (devel - optimization)"
		# mount_smbfs //avazquez:d3uxAsaX@dx-bos-file01/Devel/Optimization ~/mnt/dataxu.smb.devel

		# Mounting analytics04 drives
		echo "Mounting dataxu @ analytics04"
		sshfs dataxu@analytics04.sldc.dataxu.net:/ /Users/avazquez/mnt/dataxu.analytics04 -o IdentityFile=/Users/avazquez/.ssh/dx_general_id_rsa -o transform_symlinks -o Ciphers=arcfour -o Compression=no -oauto_cache,reconnect,defer_permissions

    # ================================================================
		# rts-updater01
		# echo "Mounting dataxu @ rts-updater01"
		# sshfs dataxu@rts-updater01.sliad.dataxu.net:/ /Users/avazquez/mnt/dataxu.rts-updater01.sliad.dataxu.net/ -o IdentityFile=/Users/avazquez/.ssh/dx_general_id_rsa -o transform_symlinks -o Ciphers=arcfour -o Compression=no -oauto_cache,reconnect,defer_permissions
    # ================================================================
		# se-tools01
		# echo "Mounting dataxu @ se-tools01"
		# sshfs dataxu@se-tools01.sldc.dataxu.net:/ /Users/avazquez/mnt/dataxu.se-tools01.sldc.dataxu.net -o IdentityFile=/Users/avazquez/.ssh/dx_general_id_rsa -o transform_symlinks -o Ciphers=arcfour -o Compression=no -oauto_cache,reconnect,defer_permissions
    # ================================================================
}

# update_hourly_data_cache() {
# 		mount_data_cache
# 		(cd /Users/avazquez/mnt/dataxu.smb.devel/amelio/daily && ./update_hourly_snapshots.sh)
# }

function update_data_snapshots_cache() {
    $PATH_TO_OPTIMIZATION_SANDBOX_REPOSITORY/init/update_snapshots.sh $1
}
